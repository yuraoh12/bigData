{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzh7woFMeXbABCjwK4hCOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chacha86/pythonai2/blob/main/%ED%86%B5%EA%B3%84_%EC%A0%95%EB%B3%B4_%EA%B0%80%EC%A0%B8%EC%98%A4%EA%B8%B0_%ED%92%80%EC%9D%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 패키지 불러오기\n",
        "import requests\n",
        "import selenium\n",
        "from bs4 import BeautifulSoup\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "options.add_experimental_option(\"useAutomationExtension\", False)\n",
        "options.add_argument('headless')\n",
        "service = ChromeService(executable_path='chromedriver.exe')\n",
        "driver = webdriver.Chrome(service=service, options=options)\n",
        "\n",
        "# 실습1\n",
        "# 뉴스의 링크를 넘기면 해당 뉴스의 댓글 통계 정보를 딕셔너리로 반환하는 함수 만들기\n",
        "\n",
        "def get_stat_gen(url) :\n",
        "    driver = webdriver.Chrome(service=service, options=options)\n",
        "    driver.get(url)\n",
        "\n",
        "    chart_sex = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.CLASS_NAME, \"u_cbox_chart_sex\"))\n",
        "    )\n",
        "#     chart_sex = driver.find_element(By.CLASS_NAME, 'u_cbox_chart_sex')\n",
        "    per_list = chart_sex.find_elements(By.CLASS_NAME, 'u_cbox_chart_per')\n",
        "\n",
        "    dic_keys = ['남자', '여자']\n",
        "    sex_dic = {}\n",
        "\n",
        "    for key, per in zip(dic_keys, per_list) :\n",
        "        sex_dic[key] = per.text\n",
        "\n",
        "    return sex_dic\n",
        "\n",
        "def get_stat_age(url) :\n",
        "\n",
        "    driver = webdriver.Chrome(service=service, options=options)\n",
        "    driver.get(url)\n",
        "    chart_age = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.CLASS_NAME, \"u_cbox_chart_age\"))\n",
        "    )\n",
        "    per_list = chart_age.find_elements(By.CLASS_NAME, 'u_cbox_chart_per')\n",
        "\n",
        "    age_keys = ['10대', '20대', '30대', '40대', '50대', '60대']\n",
        "    age_dic = {}\n",
        "\n",
        "    for key, per in zip(age_keys, per_list) :\n",
        "        age_dic[key] = per.text\n",
        "\n",
        "    return age_dic\n",
        "\n",
        "def get_reply_stat(url) :\n",
        "\n",
        "    news_id = url.split('?')[0].split('/')[-1]\n",
        "\n",
        "    gen_info = get_stat_gen(url)\n",
        "    age_info = get_stat_age(url)\n",
        "\n",
        "    gen_info.update(age_info)\n",
        "    gen_info['id'] = news_id\n",
        "\n",
        "\n",
        "    return gen_info\n",
        "\n",
        "url = 'https://n.news.naver.com/article/025/0003307263?ntype=RANKING'\n",
        "stat_info = get_reply_stat(url)\n",
        "print(stat_info)\n",
        "\n",
        "\n",
        "import news_util as ut\n",
        "\n",
        "\n",
        "def get_news_by_journal(journal) :\n",
        "    target_link = journal['link'] + '&date=20230907'\n",
        "\n",
        "    page = ut.get_page_by_url(target_link)\n",
        "    ranking_news = ut.get_rankingnews_by_page(page)\n",
        "\n",
        "    return ranking_news\n",
        "\n",
        "sbs = ut.get_journal_by_name('SBS')\n",
        "kbs = ut.get_journal_by_name('KBS')\n",
        "mbc = ut.get_journal_by_name('MBC')\n",
        "\n",
        "journal_list = [sbs, kbs, mbc]\n",
        "\n",
        "for journal in journal_list :\n",
        "    ranking_news = get_news_by_journal(journal)\n",
        "    file_path = journal['name'] + '_news.json'\n",
        "    ut.save_to_json(file_path, ranking_news)\n",
        "\n",
        "\n",
        "# 언론사 넘기면 해당 언론사의 랭킹뉴스 20개의 통계 정보를 파일로 저장하는 함수\n",
        "def get_stat_by_journal(journal) :\n",
        "    ranking_news = get_news_by_journal(journal) # 언론사의 20개 랭킹 뉴스 가져오기\n",
        "\n",
        "    stat_list = []\n",
        "    for news in ranking_news: # 각 20개의 뉴스 링크로 들어가서 통계정보 긁어오기\n",
        "        try :\n",
        "            stat_info = get_reply_stat(news['link'])\n",
        "            stat_list.append(stat_info)\n",
        "        except : # 긁어오다 문제 생기면 그냥 넘어가기\n",
        "            print(\"문제가 발생.. 아마도 대상을 찾지 못한 듯. 일단 넘어감\")\n",
        "    ut.save_to_json(f\"{journal['name']}_stat.json\", stat_list) # 20개 통계정보 다 찾으면 파일로 저장\n",
        "    print(f\"{journal['name']} 스크랩 작업이 완료되었습니다.\") # 알림\n"
      ],
      "metadata": {
        "id": "B_sKgPg3X6pQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}