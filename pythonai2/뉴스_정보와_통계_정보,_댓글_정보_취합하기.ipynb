{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSrYtdbCvwGTxLgg1+lEZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chacha86/pythonai2/blob/main/%EB%89%B4%EC%8A%A4_%EC%A0%95%EB%B3%B4%EC%99%80_%ED%86%B5%EA%B3%84_%EC%A0%95%EB%B3%B4%2C_%EB%8C%93%EA%B8%80_%EC%A0%95%EB%B3%B4_%EC%B7%A8%ED%95%A9%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwScHK1phiaZ"
      },
      "outputs": [],
      "source": [
        "# 실습3\n",
        "# 각 뉴스의 댓글 수를 스크랩해서 저장해주세요.\n",
        "\n",
        "\n",
        "## requests로 정적데이터 인지 한번 확인해보기\n",
        "url = 'https://n.news.naver.com/article/015/0004891823?ntype=RANKING'\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "## 파이썬이 브라우저인 척 하기 위함\n",
        "headers = {\n",
        "    'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "## 네이버 랭킹 뉴스 웹페이지 요청. 성공하면 응답이옴(r)\n",
        "r = requests.get(url, headers=headers)\n",
        "\n",
        "## 뷰티풀숩으로 파싱\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "target = soup.select_one('.u_cbox_count')\n",
        "\n",
        "## 결과가 None이 나옴. 동적 데이터임을 알 수 있음.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 셀레니움으로 가져오기\n",
        "\n",
        "# 패키지 불러오기\n",
        "import requests\n",
        "import selenium\n",
        "from bs4 import BeautifulSoup\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "options.add_experimental_option(\"useAutomationExtension\", False)\n",
        "options.add_argument('headless')\n",
        "service = ChromeService(executable_path='chromedriver.exe')\n",
        "driver = webdriver.Chrome(service=service, options=options)\n",
        "\n",
        "## 댓글 수 하나 가져오기 테스트\n",
        "driver.get('https://n.news.naver.com/article/015/0004891823?ntype=RANKING')\n",
        "\n",
        "count = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \".u_cbox_count\"))\n",
        "    )\n",
        "\n",
        "## 성공"
      ],
      "metadata": {
        "id": "s1nG0m30hu5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 기존 모듈을 활용하여 뉴스들의 댓글 정보 가져와서 저장하기\n",
        "\n",
        "import news_util as ut\n",
        "\n",
        "def get_news_by_journal(journal) :\n",
        "    target_link = journal['link'] + '&date=20230907'\n",
        "\n",
        "    page = ut.get_page_by_url(target_link)\n",
        "    ranking_news = ut.get_rankingnews_by_page(page)\n",
        "\n",
        "    return ranking_news\n",
        "\n",
        "sbs = ut.get_journal_by_name('SBS')\n",
        "kbs = ut.get_journal_by_name('KBS')\n",
        "mbc = ut.get_journal_by_name('MBC')\n",
        "\n",
        "journal_list = [sbs, kbs, mbc]\n",
        "\n",
        "for journal in journal_list :\n",
        "    ranking_news = get_news_by_journal(journal)\n",
        "\n",
        "    repl_cnt = {}\n",
        "    repl_cnt_list = []\n",
        "    for news in ranking_news:\n",
        "        try :\n",
        "            driver.get(news['link'])\n",
        "            count = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, \".u_cbox_count\"))\n",
        "                )\n",
        "\n",
        "            repl_cnt['count'] = count.text\n",
        "            repl_cnt['id'] = news['id']\n",
        "\n",
        "            repl_cnt_list.append(repl_cnt)\n",
        "        except :\n",
        "            print(\"문제 발생.\")\n",
        "\n",
        "    print(f\"{journal['name']} 작업 완료\")\n",
        "\n",
        "    file_path = journal['name'] + '_replCnt.json'\n",
        "    ut.save_to_json(file_path, repl_cnt_list)"
      ],
      "metadata": {
        "id": "qmvKNJD1h1fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 스크랩한 뉴스 데이터를 판다스로 불러와서 취합하기\n",
        "import pandas as pd\n",
        "\n",
        "kbs_news = pd.read_json('data/KBS_news.json')\n",
        "sbs_news = pd.read_json('data/SBS_news.json')\n",
        "mbc_news = pd.read_json('data/MBC_news.json')\n",
        "\n",
        "news_df = pd.concat([kbs_news, sbs_news, mbc_news])\n",
        "news_df"
      ],
      "metadata": {
        "id": "Gq50Lzxuh61R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 스크랩한 통계 데이터를 판다스로 불러와서 취합하기\n",
        "\n",
        "kbs_stat = pd.read_json('data/KBS_stat.json')\n",
        "sbs_stat = pd.read_json('data/SBS_stat.json')\n",
        "mbc_stat = pd.read_json('data/MBC_stat.json')\n",
        "\n",
        "stat_df = pd.concat([kbs_stat, sbs_stat, mbc_stat])\n",
        "\n",
        "stat_df"
      ],
      "metadata": {
        "id": "dP5jyf9diCNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 간이 문제. 뉴스 정보와 통계 정보를 id를 기준으로 merge해주세요."
      ],
      "metadata": {
        "id": "FUrPTiCpiFZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}