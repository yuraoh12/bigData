{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNN/kqCfI8qRxX2Mcvvsm8F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chacha86/pythonai2/blob/main/chatbot_exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4dFgbsY2c4Z"
      },
      "outputs": [],
      "source": [
        "## 챗봇 트레이닝용 데이터 얻기 -> korpora에서 다운로드 받기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 필요한 라이브러리 임포트\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "LNID98sv2wCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 데이터 불러오기\n",
        "\n",
        "df = pd.read_csv('/content/ChatbotData.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmZJNSwe26sw",
        "outputId": "f7f26a18-e5b3-485e-94cc-0323a9874070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11823 entries, 0 to 11822\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Q       11823 non-null  object\n",
            " 1   A       11823 non-null  object\n",
            " 2   label   11823 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 277.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변 문장을 따로 저장\n",
        "\n",
        "texts = [] # 질문 저장 리스트\n",
        "pairs = [] # 답변 저장 리스트\n",
        "\n",
        "for text, pair in zip(df['Q'], df['A']) :\n",
        "  texts.append(text)\n",
        "  pairs.append(pair)"
      ],
      "metadata": {
        "id": "VBQyNYbp3ZIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변 쌍을 5개 확인\n",
        "for text, pair in zip(texts[:5], pairs[:5]):\n",
        "  print(text, ' : ', pair)"
      ],
      "metadata": {
        "id": "0Y4tACGJ4F8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 빠르고 간단한 테스트를 위해 특수문자와 영어 제거\n",
        "import re\n",
        "\n",
        "def clean_sentence(sentence) :\n",
        "  sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]', r'', sentence)\n",
        "  return sentence\n",
        "\n",
        "## 함수 잘 작동하는지 테스트\n",
        "sentence = clean_sentence('12시 땡^^!!??')\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TKZKIHvX4HWe",
        "outputId": "b0216a87-7ec6-48e8-892c-eb1b8d7c20f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'12시 땡'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 한국어 문장을 분해하기 위한 라이브러리(형태소 분석)\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "gOZxNWiU4l96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 형태소 추출\n",
        "### 형태소란? 의미를 가지는 요소로서는 더 이상 쪼갤 수 없는 가장 작은 말의 단위\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "def process_morph(sentence):\n",
        "  return ' '.join(okt.morphs(sentence))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4tGe-Aj74pf4",
        "outputId": "14dd4a42-9b0b-448b-eb67-e393bc057f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요 저 는 홍길동 입니다 . 당신 의 성공 을 항상 기원 합니다 . 사랑 합니다 .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 위 함수 잘 작동하는지 확인\n",
        "process_morph('안녕하세요 저는 홍길동입니다. 당신의 성공을 항상 기원합니다. 사랑합니다.')"
      ],
      "metadata": {
        "id": "QmeL3xd64sAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 문장을 입력받아 형태소로 쪼개주는 함수\n",
        "def clean_and_morph(sentence, is_question=True):\n",
        "  ## 한글만 남기기\n",
        "  sentence = clean_sentence(sentence)\n",
        "\n",
        "  ## 형태소로 쪼개기\n",
        "  sentence = process_morph(sentence)\n",
        "\n",
        "  if is_question:\n",
        "    return sentence\n",
        "\n",
        "  else :\n",
        "    ## 후에 토크나이저하기 위해서는 공백이 꼭 들어가야 함.\n",
        "    return ('<S> ' + sentence, sentence + ' <E>')"
      ],
      "metadata": {
        "id": "ENlAtzYB42bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_txt = '안녕하세요 저는 홍길동입니다. 당신의 성공을 항상 기원합니다. 사랑합니다.'\n",
        "clean_and_morph(test_txt, is_question=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8FaA_Il48gv",
        "outputId": "55200b18-2527-4ea1-ac92-413b54a64d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<S>안녕하세요 저 는 홍길동 입니다 당신 의 성공 을 항상 기원 합니다 사랑 합니다',\n",
              " '안녕하세요 저 는 홍길동 입니다 당신 의 성공 을 항상 기원 합니다 사랑 합니다<E>')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts, pairs):\n",
        "  questions = [] # 질문 전처리 결과\n",
        "  answer_in = [] # 답변 입력 전처리 결과\n",
        "  answer_out = [] # 답변 출력 전처리 결과\n",
        "\n",
        "  ## 질문에 대한 전처리\n",
        "  for text in texts :\n",
        "    question = clean_and_morph(text, is_question=True)\n",
        "    questions.append(question)\n",
        "\n",
        "  ## 답변에 대한 전처리\n",
        "  for pair in pairs:\n",
        "    in_, out_ = clean_and_morph(pair, is_question=False) # 디코더에 들어가는 인풋과 아웃풋.\n",
        "    answer_in.append(in_)\n",
        "    answer_out.append(out_)\n",
        "\n",
        "  return questions, answer_in, answer_out"
      ],
      "metadata": {
        "id": "BQokw7IS5D4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q, ai, ao = preprocess(texts[:3], pairs[:3]) # 전처리 함수 테스트\n",
        "for s1, s2, s3 in zip(q, ai, ao) :\n",
        "  print(s1, ':' , s2, ':' , s3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScTiAgtw7Xoo",
        "outputId": "f5cf8933-0607-49b5-85c5-6f7b09c26b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12시 땡 : <S> 하루 가 또 가네요 : 하루 가 또 가네요 <E>\n",
            "1 지망 학교 떨어졌어 : <S> 위로 해 드립니다 : 위로 해 드립니다 <E>\n",
            "3 박 4일 놀러 가고 싶다 : <S> 여행 은 언제나 좋죠 : 여행 은 언제나 좋죠 <E>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = texts[:1000]\n",
        "test_pairs = pairs[:1000]\n",
        "\n",
        "questions, answers_in, answers_out = preprocess(test_texts, test_pairs)"
      ],
      "metadata": {
        "id": "yKN95mdC7cBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers_in[:5]\n",
        "answers_out[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfMxW0GL78Ht",
        "outputId": "db02c715-1c35-46b3-cf09-022f1a066362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['하루 가 또 가네요 <E>',\n",
              " '위로 해 드립니다 <E>',\n",
              " '여행 은 언제나 좋죠 <E>',\n",
              " '여행 은 언제나 좋죠 <E>',\n",
              " '눈살 이 찌푸려지죠 <E>']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 후에 토크나이저를 한번에 하기 위해 문장을 합쳐줌(리스트 합치기)\n",
        "all_sentences = questions + answers_in + answers_out"
      ],
      "metadata": {
        "id": "V2I6iBMi8IRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 형태소 개수 -> 이거 왜 하지?\n",
        "a = (' '.join(questions) + ' '.join(answers_in) + ' '.join(answers_out)).split()\n",
        "len(set(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0pMjOZI8SGf",
        "outputId": "f62336f3-6d9a-4d84-ab2b-3070e5acb1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2300"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "MW_wLigY8UXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## filter => 문장의 특수기호등을 임의로 처리하지 말라. 필터링 하지 말라\n",
        "## lower => 소문자로 변경하지 마라\n",
        "## oov_token => 단어 사전에 존재하지 않는 단어라면 ''로 대체\n",
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')"
      ],
      "metadata": {
        "id": "X32S5G8g9i5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 단어 사전 만들기\n",
        "## 공백을 기준으로 쪼개주는 듯하다\n",
        "tokenizer.fit_on_texts(all_sentences)"
      ],
      "metadata": {
        "id": "Epk5qlHN92VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 각 단어들에 넘버링(벡터화) -> 고유한 단어만 남기기 때문에 2300개임\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrJ9-1K1-GEG",
        "outputId": "999308a6-8fcb-4eff-cea6-2fb7d575d5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2300"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 각 단어와 단어의 인덱스 번호를 확인\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "  print(f'{word}\\t\\t => \\t{idx}')\n",
        "  if idx > 10:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECQyLpy0-Yam",
        "outputId": "4be76805-ee5d-4d6a-8fbf-111bf3e6aa29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<OOV>\t\t => \t1\n",
            "<S>\t\t => \t2\n",
            "<E>\t\t => \t3\n",
            "이\t\t => \t4\n",
            "거\t\t => \t5\n",
            "을\t\t => \t6\n",
            "가\t\t => \t7\n",
            "나\t\t => \t8\n",
            "예요\t\t => \t9\n",
            "사람\t\t => \t10\n",
            "요\t\t => \t11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1 # <OOV> 까지 합하면 +1 해야함\n",
        "VOCAB_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9bIAu2z-gPW",
        "outputId": "5cfec28d-7c18-4c3a-ec55-2b6d892f1740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2301"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "\n",
        "for idx in question_sequences[2]:\n",
        "  print(tokenizer.index_word[idx])"
      ],
      "metadata": {
        "id": "-mEaAghm-rjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_in_sequences = tokenizer.texts_to_sequences(answers_in)\n",
        "answer_out_sequences = tokenizer.texts_to_sequences(answers_out)"
      ],
      "metadata": {
        "id": "sYQZM0pC-y8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[0], answer_in_sequences[0],answer_out_sequences[0], pairs[0], tokenizer.word_index['하루']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AKqyHOY_T8U",
        "outputId": "93ede61c-bbd1-431e-d9a8-999423defb27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('12시 땡', [2, 391, 7, 356, 1234], [391, 7, 356, 1234, 3], '하루가 또 가네요.', 391)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 딥러닝의 경우 입력값이 항상 일정해야 하므로(네트워크 모델은 입력값에 의해 모양이 바뀌므로 입력값은 바뀌면 안된다.)\n",
        "MAX_LENGTH = 30 # 최대 몇개의 단어\n",
        "TRUNCATING = 'post' # 문장이 MAX_LENGTH를 넘어갈 때 앞(pre), 뒤(post) 어디를 자를지 여부\n",
        "PADDING = 'post' # 문장이 MAX_LENGTH를 채우지 못했을 때 앞(pre), 뒤(post) 어디로 채울지 여부"
      ],
      "metadata": {
        "id": "-mYfvXrs_X72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 트런케이팅과 패딩 적용하기\n",
        "question_padded = pad_sequences(question_sequences, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_in_padded = pad_sequences(answer_in_sequences, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_out_padded = pad_sequences(answer_out_sequences, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)"
      ],
      "metadata": {
        "id": "2KwHAoil_kzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 잘 적용 되었는지 shape로 확인\n",
        "question_padded.shape, answer_in_padded.shape, answer_out_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg_GiKfNALzN",
        "outputId": "79282861-9b33-4be2-b2a6-679163c3b26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 30), (1000, 30), (1000, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 12 땡이라는 문장이기 때문에 앞에 2개만 존재하고 나머지 28개는 padding 때문에 비어 있음.\n",
        "question_padded[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sqjElpsAOGd",
        "outputId": "0649de93-2369-401e-8e5c-e1eb457fa91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1608, 1609,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 단어에는 비교우위가 없으므로 카테고리컬로 데이터로 보고 원핫 인코딩을 해준다.\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 우선, 단어 인덱스의 최대값을 찾아야 합니다.\n",
        "# 이 값이 원-핫 벡터의 길이가 됩니다.\n",
        "\n",
        "# 원-핫 인코딩 수행\n",
        "data_one_hot = to_categorical(answer_in_padded, num_classes=VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "nM4fd0YCAk0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_one_hot.shape\n",
        "data_one_hot"
      ],
      "metadata": {
        "id": "fqF4HSxsBydV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델이 예측한 인코딩된 값을 다시 문자로 디코딩 해주는 함수\n",
        "def convert_index_to_text(indexes, end_token):\n",
        "  sentence = ''\n",
        "  for index in indexes: ## 문장의 순서\n",
        "    if index == end_token:  ## 문장의 마지막이면 종료\n",
        "      break\n",
        "    if index > 0 and tokenizer.index_word[index] is not None: ## 단아 사전에 존재하고 올바른 인덱스라면\n",
        "      sentence += tokenizer.index_word[index] # 최종 문자열에 이어 붙인다.\n",
        "    else:\n",
        "      sentence += '' # 없는 거면 공백.\n",
        "\n",
        "    sentence += ' ' # 한 형태소가 끝나면 띄어쓰기\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "HTABEZR7BzxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "## Embbeding -> 카테고리컬 단어값을 고차원으로 바꾸는 것.\n",
        "## Embbeding을 해야 하는 이유\n",
        "### 1. 차원의 저주. 원핫 인코딩의 경우 어마어마한 차원이 생성됨. 이를 줄이기 위해 임베딩 공간을 사용\n",
        "### 2. 단어간의 의미와 유사성 추론 가능. 유사한 단어들 간의 벡터 거리를 계산해 유사도 측정.\n",
        "### 3. sparse 벡터를 dense 벡터로 변환하여 사용하므로 성능 이점.\n",
        "\n",
        "## LSTM -> Long Short Term Memory(순환 신경망의 일종)\n",
        "### RNN은 이전 셀의 내용을 다음 셀이 전이하는데 이 전이 과정이 길어질 수록 기억이 사라지는 현상이 있다.(그래디언트 소실 때문) 시그모이드나 하이퍼볼릭 탄젠트는 미분 될 때마다 기울기가 작아진다.(출력값이 -1 ~ 0 ~ 1 사이이므로. 기울기를 곱하면 작은 값이나 0이 된다.)\n",
        "### LSTM은 기억을 장기 보존하기 위해 게이트를 구성한 버전 -> 어떻게 해결했나? 여러 게이트를 두어 게이트에 추가 정보를 저장. 학습 과정에서 추가 정보를 살릴지 말지. 얼마나 반영할지 결정한다.\n",
        "\n",
        "\n",
        "## Dense -> 뉴런 layer를 의미. 뉴런의 개수와 활성화 함수를 정할 수 있다.\n",
        "\n",
        "## Dropout -> 딥러닝의 규제 방법. 하이퍼 파라미터. 과도한 뉴런은 과적합을 유발할 수 있기 때문에, x% 만큼 뉴런을 비활성화 시켜 일반화 성능을 높인다.\n",
        "\n",
        "## Model\n",
        "### 텐서플로 기본 신경망 모델. 여러 Layer를 정의할 수 있고, compile 메서드를 이용해 최적화 알고리즘 및 손실함수, 평가지표등을 정할 수 있다.\n",
        "### 예측과 모델 저장 및 로드도 가능.\n",
        "### 서브 클래싱 -> 상속 받아 사용자 정의 모델도 생성 가능. 여기서는 서브클래싱용으로 사용.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ojo9QYLDCT37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 모델 객체\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    ## Embedding -> 카테고리컬 단어값을 고차원으로 바꾸는 것(우리는 원핫을 사용함)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps) ## 단어 개수, 변환하고자 하는 임베딩 차원, 한 문장의 길이\n",
        "    self.dropout = Dropout(0.2) ## 과적합을 방지하기 위한 하이퍼파라미터. 임의로 20% 뉴런을 잡아서 비활성화 시킴\n",
        "    self.lstm = LSTM(units, return_state=True) ## 최종 히든 스테이트를 얻어야 벡터 콘텍스트에 넣을 수 있음.\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.embedding(inputs) ## 임베딩 세팅\n",
        "    x = self.dropout(x) ## 과적합 방지 파라미터 세팅\n",
        "    x, hidden_state, cell_state =self.lstm(x) ## 정답, 히든 스테이트, 셀 스테이트\n",
        "\n",
        "    return [hidden_state, cell_state]"
      ],
      "metadata": {
        "id": "RG_WGupOCa8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "    self.dropout = Dropout(0.2)\n",
        "    self.lstm = LSTM(units,\n",
        "                     return_state=True, # 스테이트값을 알아야 다음 셀에서 진행 가능\n",
        "                     return_sequences=True ## 각 유닛의 스테이트값을 다 얻어서 결과를 얻어야 하므로\n",
        "    )\n",
        "    self.dense = Dense(vocab_size, activation='softmax') ## 결과를 얻기 위한 출력층\n",
        "\n",
        "  def call(self, inputs, initial_state): ## initial_state는 encoder의 출력값\n",
        "    x = self.embedding(inputs)\n",
        "    x = self.dropout(x)\n",
        "    x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "    x = self.dense(x) # 최종 결과값은 출력층을 거쳐 결과를 낸다\n",
        "\n",
        "    return x, hidden_state, cell_state\n"
      ],
      "metadata": {
        "id": "sg0FU4UrR0xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "  def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "    super(Seq2seq, self).__init__()\n",
        "\n",
        "    self.start_token = start_token\n",
        "    self.end_token = end_token\n",
        "    self.time_steps = time_steps # 문장의 길이(30)\n",
        "\n",
        "    self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "    self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "\n",
        "  def call(self, inputs, training=True): ## training: 학습용, 예측용 구별\n",
        "    if training: ## 학습일 땐,\n",
        "      encoder_inputs, decoder_inputs = inputs ## 인코더, 디코더 모두 동일한 입력값 넣는다.\n",
        "      context_vector = self.encoder(encoder_inputs) ## 인코더에 넣어서 벡터 얻어냄\n",
        "      decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs, initial_state=context_vector) ## 얻어낸 인코더의 벡터값을 디코더에 사용\n",
        "\n",
        "      return decoder_outputs\n",
        "\n",
        "    else: ## 예측일 땐,\n",
        "      context_vector = self.encoder(inputs) ##\n",
        "      target_seq = tf.constant([[self.start_token]], dtype=tf.float32) ## 첫번째는 무조건 ,\n",
        "      results = tf.TensorArray(tf.int32, self.time_steps) ## 결과 배열. 그래프 그리기 위해 텐서 배열로 담는다.\n",
        "\n",
        "      for i in tf.range(self.time_steps):\n",
        "        decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq, initial_state=context_vector)\n",
        "        decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
        "        decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "        results = results.write(i, decoder_output)\n",
        "\n",
        "        if decoder_output == self.end_token:\n",
        "          break\n",
        "\n",
        "        target_seq = decoder_output\n",
        "        context_vector = [decoder_hidden, decoder_cell]\n",
        "\n",
        "      return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "metadata": {
        "id": "bcuVdL-zSAM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = MAX_LENGTH\n",
        "\n",
        "START_TOKEN = tokenizer.word_index['']\n",
        "END_TOKEN = tokenizer.word_index['']\n",
        "UNITS = 128\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "DATA_LENGTH = len(questions)\n",
        "SAMPLE_SIZE = 3\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "1E42oD_lE-M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'sample-checkpoint.h5'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_best_only=True,\n",
        "                             monitor='loss',\n",
        "                             verbose=1,\n",
        "                             save_weights_only=True)"
      ],
      "metadata": {
        "id": "RANr0H1UFAKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)\n",
        "seq2seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "AQBKkMpeE-4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"processing epoch : {epoch * 10 + 1} ...\")\n",
        "  seq2seq.fit([question_padded, answer_in_padded], answer_out_one_hot, epochs=10, batch_size=BATCH_SIZE, callbacks=[checkpoint])\n",
        "\n",
        "  samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "  for idx in samples:\n",
        "    question_inputs = question_padded[idx]\n",
        "    results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "\n",
        "    results = convert_index_to_text(results, END_TOKEN)"
      ],
      "metadata": {
        "id": "5W2NQGMkFGZa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}